{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ResNet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"梯度消失和梯度下降：\n\n当初始化的网络权值小于1，当层数增多时，小于1的值不断相乘，最后就导致梯度消失的情况出现。同理，当权值过大时，最后大于1的值不断相乘，就会产生梯度爆炸。","metadata":{}},{"cell_type":"markdown","source":"网络之所以难以训练，是因为存在着梯度消失的问题，离 loss 函数越远的层，在反向传播的时候，梯度越小，就越难以更新，随着层数的增加，这个现象越严重。之前有两种常见的方案来解决这个问题：\n\n1.按层训练，先训练比较浅的层，然后在不断增加层数，但是这种方法效果不是特别好，而且比较麻烦\n\n2.使用更宽的层，或者增加输出通道，而不加深网络的层数，这种结构往往得到的效果又不好\n\nResNet 通过引入了跨层链接解决了梯度回传消失的问题。","metadata":{}},{"cell_type":"markdown","source":"使用普通的连接，上层的梯度必须要一层一层传回来，而是用残差连接，相当于中间有了一条更短的路，梯度能够从这条更短的路传回来，避免了梯度过小的情况。\n\n假设某层的输入是 x，期望输出是 H(x)， 如果我们直接把输入 x 传到输出作为初始结果，这就是一个更浅层的网络，更容易训练，而这个网络没有学会的部分，我们可以使用更深的网络 F(x) 去训练它，使得训练更加容易，最后希望拟合的结果就是 F(x) = H(x) - x，这就是一个残差的结构\n\n残差网络的结构就是上面这种残差块的堆叠，下面让我们来实现一个 residual block","metadata":{}},{"cell_type":"markdown","source":"一如既往的导入库，写训练函数，读取即预处理数据集","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torchvision.datasets import mnist\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:47:04.399500Z","iopub.execute_input":"2021-10-05T08:47:04.399977Z","iopub.status.idle":"2021-10-05T08:47:09.071543Z","shell.execute_reply.started":"2021-10-05T08:47:04.399850Z","shell.execute_reply":"2021-10-05T08:47:09.070537Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().data\n    return num_correct / total\n\n\ndef train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n    if torch.cuda.is_available():\n        net = net.cuda()\n    prev_time = datetime.now()\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            if torch.cuda.is_available():\n                im = Variable(im.cuda())  # (bs, 3, h, w)\n                label = Variable(label.cuda())  # (bs, h, w)\n            else:\n                im = Variable(im)\n                label = Variable(label)\n            # forward\n            output = net(im)\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.data\n            train_acc += get_acc(output, label)\n\n        cur_time = datetime.now()\n        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n        m, s = divmod(remainder, 60)\n        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            for im, label in valid_data:\n                if torch.cuda.is_available():\n                    im = Variable(im.cuda())\n                    label = Variable(label.cuda())\n                else:\n                    im = Variable(im)\n                    label = Variable(label)\n                output = net(im)\n                loss = criterion(output, label)\n                valid_loss += loss.data\n                valid_acc += get_acc(output, label)\n            epoch_str = (\n                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n                % (epoch, train_loss / len(train_data),\n                   train_acc / len(train_data), valid_loss / len(valid_data),\n                   valid_acc / len(valid_data)))\n        else:\n            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %\n                         (epoch, train_loss / len(train_data),\n                          train_acc / len(train_data)))\n        prev_time = cur_time\n        print(epoch_str + time_str)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:47:09.075805Z","iopub.execute_input":"2021-10-05T08:47:09.076026Z","iopub.status.idle":"2021-10-05T08:47:09.092577Z","shell.execute_reply.started":"2021-10-05T08:47:09.075998Z","shell.execute_reply":"2021-10-05T08:47:09.091433Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def data_tf(x):\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # 数据预处理，标准化\n    x=np.array([x.tolist()])\n    x = torch.from_numpy(x)    \n    return x\n\nfrom torchvision.datasets import mnist # 导入 pytorch 内置的 mnist 数据\ntrain_set = mnist.MNIST('./data', train=True, transform=data_tf,download=True) # 重新载入数据集，申明定义的数据变换\ntest_set = mnist.MNIST('./data', train=False, transform=data_tf,download=True)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\ntest_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:47:09.094296Z","iopub.execute_input":"2021-10-05T08:47:09.095292Z","iopub.status.idle":"2021-10-05T08:47:10.551904Z","shell.execute_reply.started":"2021-10-05T08:47:09.095235Z","shell.execute_reply":"2021-10-05T08:47:10.550751Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"查看一个batch数据的尺寸，便于网络的设计","metadata":{}},{"cell_type":"code","source":"x=torch.tensor([])\nfor i,j in train_data:\n#     print(i[0].shape) \n#     print(i[0])\n    x=i\n    break\nprint(x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:47:10.554725Z","iopub.execute_input":"2021-10-05T08:47:10.555173Z","iopub.status.idle":"2021-10-05T08:47:10.605142Z","shell.execute_reply.started":"2021-10-05T08:47:10.555099Z","shell.execute_reply":"2021-10-05T08:47:10.603990Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"接下来设计实现一个 residual block","metadata":{}},{"cell_type":"code","source":"# 利用pytorch的卷积函数，固定kernel_size参数，以定义一个3*3的卷积函数\ndef conv3x3(in_channel, out_channel, stride=1):\n    return nn.Conv2d(in_channel, out_channel, 3, stride=stride, padding=1, bias=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:47:10.606880Z","iopub.execute_input":"2021-10-05T08:47:10.607521Z","iopub.status.idle":"2021-10-05T08:47:10.612791Z","shell.execute_reply.started":"2021-10-05T08:47:10.607482Z","shell.execute_reply":"2021-10-05T08:47:10.611760Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"尝试用面向对象的写法，之后多采用面向对象的写法！！！","metadata":{}},{"cell_type":"code","source":"class residual_block(nn.Module):\n    def __init__(self, in_channel, out_channel, same_shape=True):\n        super(residual_block, self).__init__()\n        \n#         如果要保持tensor的shape不变，则卷积核的滑动步长改变为2，可达到一个通道数倍增，图像长宽减半的效果\n        self.same_shape = same_shape\n        stride=1 if self.same_shape else 2\n        \n        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n#         每次卷积后进行一次批标准化\n        self.bn1 = nn.BatchNorm2d(out_channel)\n        \n        self.conv2 = conv3x3(out_channel, out_channel)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        if not self.same_shape:\n            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        out = F.relu(self.bn1(out), True)\n        out = self.conv2(out)\n        out = F.relu(self.bn2(out), True)\n        \n        if not self.same_shape:\n            x = self.conv3(x)\n        return F.relu(x+out, True)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:47:10.615093Z","iopub.execute_input":"2021-10-05T08:47:10.615950Z","iopub.status.idle":"2021-10-05T08:47:10.628344Z","shell.execute_reply.started":"2021-10-05T08:47:10.615816Z","shell.execute_reply":"2021-10-05T08:47:10.627203Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(\"1:\",x.shape)\nx=nn.Conv2d(1, 32, 3, 1)(x.float())\nprint(\"2:\",x.shape)\n\nx=nn.MaxPool2d(2, 2)(x)\nprint(\"3:\",x.shape)\n\nx=residual_block(32,32)(x)\nx=residual_block(32,32)(x)\nprint(\"4:\",x.shape)\n\nx=residual_block(32,64,False)(x)\nx=residual_block(64,64)(x)\nprint(\"5:\",x.shape)\n\n\nx=residual_block(64,128,False)(x)\nx=residual_block(128,128)(x)\nprint(\"6:\",x.shape)\n\n\nx=residual_block(128,256,False)(x)\nx=residual_block(256,256)(x)\nprint(\"7:\",x.shape)\n\n\nx=nn.AvgPool2d(2)(x)\nprint(\"8:\",x.shape)\n\nx = x.view(x.shape[0], -1)\nprint(\"9:\",x.shape)\n\nx=nn.Linear(256, 10)(x)\nprint(\"10:\",x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:47:10.630213Z","iopub.execute_input":"2021-10-05T08:47:10.630607Z","iopub.status.idle":"2021-10-05T08:47:10.893730Z","shell.execute_reply.started":"2021-10-05T08:47:10.630551Z","shell.execute_reply":"2021-10-05T08:47:10.892839Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"ResNet是 residual block 模块的堆叠","metadata":{}},{"cell_type":"code","source":"class resnet(nn.Module):\n    def __init__(self, in_channel, num_classes, verbose=False):\n        super(resnet, self).__init__()\n        self.verbose = verbose\n        \n        self.block1 = nn.Conv2d(in_channel, 32, 3,1)\n        \n        self.block2 = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            residual_block(32, 32),\n            residual_block(32, 32)\n        )\n        \n        self.block3 = nn.Sequential(\n            residual_block(32, 64, False),\n            residual_block(64, 64)\n        )\n        \n        self.block4 = nn.Sequential(\n            residual_block(64, 128, False),\n            residual_block(128, 128)\n        )\n        \n        self.block5 = nn.Sequential(\n            residual_block(128, 256, False),\n            residual_block(256, 256),\n            nn.AvgPool2d(2)\n        )\n        \n        self.classifier = nn.Linear(256, num_classes)\n        \n    def forward(self, x):\n        x=x.float()\n        x = self.block1(x)\n        if self.verbose:\n            print('block 1 output: {}'.format(x.shape))\n        x = self.block2(x)\n        if self.verbose:\n            print('block 2 output: {}'.format(x.shape))\n        x = self.block3(x)\n        if self.verbose:\n            print('block 3 output: {}'.format(x.shape))\n        x = self.block4(x)\n        if self.verbose:\n            print('block 4 output: {}'.format(x.shape))\n        x = self.block5(x)\n        if self.verbose:\n            print('block 5 output: {}'.format(x.shape))\n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:47:10.895551Z","iopub.execute_input":"2021-10-05T08:47:10.895883Z","iopub.status.idle":"2021-10-05T08:47:10.909974Z","shell.execute_reply.started":"2021-10-05T08:47:10.895842Z","shell.execute_reply":"2021-10-05T08:47:10.908740Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"net = resnet(1, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-1)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:47:10.912159Z","iopub.execute_input":"2021-10-05T08:47:10.912669Z","iopub.status.idle":"2021-10-05T08:47:10.956763Z","shell.execute_reply.started":"2021-10-05T08:47:10.912543Z","shell.execute_reply":"2021-10-05T08:47:10.955848Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train(net, train_data, test_data, 20, optimizer, criterion)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T08:47:10.960312Z","iopub.execute_input":"2021-10-05T08:47:10.960907Z","iopub.status.idle":"2021-10-05T08:58:48.266831Z","shell.execute_reply.started":"2021-10-05T08:47:10.960878Z","shell.execute_reply":"2021-10-05T08:58:48.264715Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"可以看到，训练20次测试集准确率可达0.99555","metadata":{}}]}