{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:32:56.45309Z","iopub.execute_input":"2021-09-28T07:32:56.453546Z","iopub.status.idle":"2021-09-28T07:32:56.479777Z","shell.execute_reply.started":"2021-09-28T07:32:56.453451Z","shell.execute_reply":"2021-09-28T07:32:56.479088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**全部实验使用 MNIST 数据集**","metadata":{}},{"cell_type":"code","source":"from torchvision.datasets import mnist # 导入 pytorch 内置的 mnist 数据\ntrain_set = mnist.MNIST('./data', train=True,  download=True) # 重新载入数据集，申明定义的数据变换\ntest_set = mnist.MNIST('./data', train=False,  download=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:32:56.48096Z","iopub.execute_input":"2021-09-28T07:32:56.481291Z","iopub.status.idle":"2021-09-28T07:33:00.192355Z","shell.execute_reply.started":"2021-09-28T07:32:56.481265Z","shell.execute_reply":"2021-09-28T07:33:00.191573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_set[0])\nprint(train_set[0][0])\nprint(type(train_set[0][0]))\ntrain_set[0][0]","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.193554Z","iopub.execute_input":"2021-09-28T07:33:00.193744Z","iopub.status.idle":"2021-09-28T07:33:00.22039Z","shell.execute_reply.started":"2021-09-28T07:33:00.193721Z","shell.execute_reply":"2021-09-28T07:33:00.219488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"可见数据是28✖28的灰度图片","metadata":{}},{"cell_type":"markdown","source":"**CNN**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.222897Z","iopub.execute_input":"2021-09-28T07:33:00.223211Z","iopub.status.idle":"2021-09-28T07:33:00.226967Z","shell.execute_reply.started":"2021-09-28T07:33:00.22317Z","shell.execute_reply":"2021-09-28T07:33:00.226093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1、卷积\n首先需要输入是一个 `torch.autograd.Variable()` 的类型，大小是 (batch, channel, H, W)，其中 batch 表示输入的一批数据的数目，第二个是输入的通道数，一般一张彩色的图片是 3，灰度图是 1，而卷积网络过程中的通道数比较大，会出现几十到几百的通道数，H 和 W 表示输入图片的高度和宽度，比如一个 batch 是 32 张图片，每张图片是 3 通道，高和宽分别是 50 和 100，那么输入的大小就是 (32, 3, 50, 100)","metadata":{}},{"cell_type":"markdown","source":"经典的Mnist数据集为(28,28)的图片集","metadata":{}},{"cell_type":"code","source":"im=train_set[2][0]\nplt.imshow(im, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.228473Z","iopub.execute_input":"2021-09-28T07:33:00.22903Z","iopub.status.idle":"2021-09-28T07:33:00.462738Z","shell.execute_reply.started":"2021-09-28T07:33:00.228992Z","shell.execute_reply":"2021-09-28T07:33:00.461908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"显然这是4","metadata":{}},{"cell_type":"markdown","source":"将图片矩阵转化为 pytorch tensor，并适配卷积输入的要求","metadata":{}},{"cell_type":"code","source":"im=np.array(im)\nim = torch.from_numpy(im.reshape((1, 1, im.shape[0],im.shape[1])))\nim.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.463905Z","iopub.execute_input":"2021-09-28T07:33:00.464586Z","iopub.status.idle":"2021-09-28T07:33:00.47274Z","shell.execute_reply.started":"2021-09-28T07:33:00.464551Z","shell.execute_reply":"2021-09-28T07:33:00.471836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 使用 nn.Conv2d\nconv1 = nn.Conv2d(1, 1, 3, bias=False) # 定义3*3的卷积模板\n\nsobel_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype='float32') # 定义轮廓检测sobel算子\nsobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # 适配卷积的输入输出\nconv1.weight.data = torch.from_numpy(sobel_kernel) # 给卷积的 kernel 赋值\n\nedge1 = conv1(im.float())# 作用在图片上\nedge1 = edge1.data.squeeze().numpy() # 将输出转换为图片的格式","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.474213Z","iopub.execute_input":"2021-09-28T07:33:00.474427Z","iopub.status.idle":"2021-09-28T07:33:00.514339Z","shell.execute_reply.started":"2021-09-28T07:33:00.474404Z","shell.execute_reply":"2021-09-28T07:33:00.513556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"从卷积后图片变小的情况来看，卷积采用的是丢弃法，可使用padding参数调整","metadata":{}},{"cell_type":"code","source":"edge1.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.515445Z","iopub.execute_input":"2021-09-28T07:33:00.515713Z","iopub.status.idle":"2021-09-28T07:33:00.52126Z","shell.execute_reply.started":"2021-09-28T07:33:00.515686Z","shell.execute_reply":"2021-09-28T07:33:00.520159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"可见边缘检测之后的结果","metadata":{}},{"cell_type":"code","source":"plt.imshow(edge1, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.523539Z","iopub.execute_input":"2021-09-28T07:33:00.523806Z","iopub.status.idle":"2021-09-28T07:33:00.721821Z","shell.execute_reply.started":"2021-09-28T07:33:00.523778Z","shell.execute_reply":"2021-09-28T07:33:00.72088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.722841Z","iopub.execute_input":"2021-09-28T07:33:00.723055Z","iopub.status.idle":"2021-09-28T07:33:00.729072Z","shell.execute_reply.started":"2021-09-28T07:33:00.723029Z","shell.execute_reply":"2021-09-28T07:33:00.728121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im=im.float()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.729972Z","iopub.execute_input":"2021-09-28T07:33:00.730168Z","iopub.status.idle":"2021-09-28T07:33:00.738912Z","shell.execute_reply.started":"2021-09-28T07:33:00.730146Z","shell.execute_reply":"2021-09-28T07:33:00.738299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2、池化层\n\n卷积网络中另外一个非常重要的结构就是池化，这是利用了图片的下采样不变性，即一张图片变小了还是能够看出了这张图片的内容，而使用池化层能够将图片大小降低，非常好地提高了计算效率，同时池化层也没有参数。池化的方式有很多种，比如最大值池化，均值池化等等，在卷积网络中一般使用最大值池化。","metadata":{}},{"cell_type":"code","source":"# 使用 nn.MaxPool2d\npool1 = nn.MaxPool2d(2, 2)\nsmall_im1 = pool1(im)\nsmall_im1 = small_im1.data.squeeze().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.740092Z","iopub.execute_input":"2021-09-28T07:33:00.740884Z","iopub.status.idle":"2021-09-28T07:33:00.759037Z","shell.execute_reply.started":"2021-09-28T07:33:00.740842Z","shell.execute_reply":"2021-09-28T07:33:00.75827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(small_im1, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.760041Z","iopub.execute_input":"2021-09-28T07:33:00.760852Z","iopub.status.idle":"2021-09-28T07:33:00.96397Z","shell.execute_reply.started":"2021-09-28T07:33:00.760812Z","shell.execute_reply":"2021-09-28T07:33:00.963143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"可以看到图片几乎没有变化，说明池化层只是减小了图片的尺寸，并不会影响图片的内容","metadata":{}},{"cell_type":"markdown","source":"****","metadata":{}},{"cell_type":"markdown","source":"> ","metadata":{}},{"cell_type":"markdown","source":"**在我们正式进入模型的构建和训练之前，对数据增加一些预处理，同时使用批标准化能够得到非常好的收敛结果，这也是卷积网络能够训练到非常深的层的一个重要原因。** ","metadata":{}},{"cell_type":"markdown","source":"在 2015 年一篇论文提出了这个方法，批标准化，简而言之，就是对于每一层网络的输出，对其做一个归一化，使其服从标准的正态分布，这样后一层网络的输入也是一个标准的正态分布，所以能够比较好的进行训练，加快收敛速度","metadata":{}},{"cell_type":"markdown","source":"3、批标准化\nbatch normalization 的实现非常简单，对于给定的一个 batch 的数据 $B = \\{x_1, x_2, \\cdots, x_m\\}$算法的公式如下\n​\n$$\n\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i\n$$\n$$\n\\sigma^2_B = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2\n$$\n$$\n\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}}\n$$\n$$\ny_i = \\gamma \\hat{x}_i + \\beta\n$$\n第一行和第二行是计算出一个 batch 中数据的均值和方差，接着使用第三个公式对 batch 中的每个数据点做标准化，$\\epsilon$ 是为了计算稳定引入的一个小的常数，通常取 $10^{-5}$，最后利用权重修正得到最后的输出结果","metadata":{}},{"cell_type":"code","source":"# 批标准化方法\ndef batch_norm_1d(x, gamma, beta, is_training, moving_mean, moving_var, moving_momentum=0.1):\n    eps = 1e-5\n    x_mean = torch.mean(x, dim=0, keepdim=True) # 保留维度进行 broadcast\n    x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True)\n    \n#     如果是训练集，则使用计算得出的均值和方差代入式子\n    if is_training:\n        x_hat = (x - x_mean) / torch.sqrt(x_var + eps)\n        \n#       算出移动平均均值和方差，与上一层输入的移动均值和方差和这层算出来的均值和方差有关\n        moving_mean[:] = moving_momentum * moving_mean + (1. - moving_momentum) * x_mean\n        moving_var[:] = moving_momentum * moving_var + (1. - moving_momentum) * x_var\n# 测试集直接使用之前训练集处理时算出来的均值和方差\n    else:\n        x_hat = (x - moving_mean) / torch.sqrt(moving_var + eps)\n    return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.965005Z","iopub.execute_input":"2021-09-28T07:33:00.965232Z","iopub.status.idle":"2021-09-28T07:33:00.97192Z","shell.execute_reply.started":"2021-09-28T07:33:00.965204Z","shell.execute_reply":"2021-09-28T07:33:00.971311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"这里的 $\\gamma$ 和 $\\beta$ 都作为参数进行训练","metadata":{}},{"cell_type":"markdown","source":"测试的时候不能用测试的数据集去算均值和方差，而是用训练的时候算出的移动平均均值和方差去代替","metadata":{}},{"cell_type":"markdown","source":"**** ","metadata":{}},{"cell_type":"markdown","source":"4、对数据进行一些必要的预处理","metadata":{}},{"cell_type":"code","source":"x = np.array(train_set[0][0], dtype='float32') / 255\nx = (x - 0.5) / 0.5 # 数据预处理，标准化\nx = x.reshape((-1,)) # 拉平\nx = torch.from_numpy(x)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.97289Z","iopub.execute_input":"2021-09-28T07:33:00.973662Z","iopub.status.idle":"2021-09-28T07:33:00.994668Z","shell.execute_reply.started":"2021-09-28T07:33:00.973618Z","shell.execute_reply":"2021-09-28T07:33:00.993638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"一列列数据进行处理比较麻烦，可以做个函数进行处理，这里使用mnist.MNIST自带的方法","metadata":{}},{"cell_type":"code","source":"def data_tf(x):\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # 数据预处理，标准化\n    x = x.reshape((-1,)) # 拉平\n    x = torch.from_numpy(x)\n    return x\ntrain_set = mnist.MNIST('./data', train=True, transform=data_tf, download=True) # 重新载入数据集，申明定义的数据变换\ntest_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:00.995624Z","iopub.execute_input":"2021-09-28T07:33:00.995841Z","iopub.status.idle":"2021-09-28T07:33:01.046169Z","shell.execute_reply.started":"2021-09-28T07:33:00.995816Z","shell.execute_reply":"2021-09-28T07:33:01.045141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**当数据量很大时，内存往往不足，使用Dataloader迭代处理数据**\nDataLoader本质上就是一个iterable（跟python的内置类型list等一样），并利用多进程来加速batch data的处理，使用yield来使用有限的内存","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\ntest_data = DataLoader(test_set, batch_size=128, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:01.047616Z","iopub.execute_input":"2021-09-28T07:33:01.047998Z","iopub.status.idle":"2021-09-28T07:33:01.053096Z","shell.execute_reply.started":"2021-09-28T07:33:01.047954Z","shell.execute_reply":"2021-09-28T07:33:01.051915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"安排好数据加载处理环节后，开始设计训练环节","metadata":{}},{"cell_type":"code","source":"class multi_network(nn.Module):\n    def __init__(self):\n        super(multi_network, self).__init__()\n#         网络设计\n# \n#       全连接层  类似于100个线性回归模型\n        self.layer1 = nn.Linear(784, 100)\n    \n#  激活函数\n\n# ======================================================================\n# 模仿大脑神经元结构，输入的数据经过计算得到结果，接着经过激活函数（兴奋或抑制），再传给下一层神经元 \n# 常用的激活函数有sigmoid、tanh、ReLU，其中最普遍最多使用的是ReLU，这里也选择使用ReLU\n# ======================================================================\n        self.relu = nn.ReLU(True)\n\n        self.layer2 = nn.Linear(100, 10)\n\n# 批处理参数随机初始化，训练过程会逐渐变化\n\n# torch.nn.init 内置了一张叫Xavier的参数初始化方法，\n# 方法来源于 2010 年的一篇论文 Understanding the difficulty of training deep feedforward neural networks，\n# 其通过数学的推到，证明了这种初始化方式可以使得每一层的输出方差是尽可能相等的\n        self.gamma = nn.init.xavier_uniform(torch.randn(100).reshape(1,100),gain=1).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n        self.beta = nn.init.xavier_uniform(torch.randn(100).reshape(1,100),gain=1).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n\n#         self.gamma = nn.Parameter(torch.randn(100))\n#         self.beta = nn.Parameter(torch.randn(100))\n\n#  移动均值和方差设为0\n        self.moving_mean = Variable(torch.zeros(100))\n        self.moving_var = Variable(torch.zeros(100))\n        \n    def forward(self, x, is_train=True):\n        x = self.layer1(x)\n        x = batch_norm_1d(x, self.gamma, self.beta, is_train, self.moving_mean, self.moving_var)\n        x = self.relu(x)\n        x = self.layer2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:01.054572Z","iopub.execute_input":"2021-09-28T07:33:01.054893Z","iopub.status.idle":"2021-09-28T07:33:01.06843Z","shell.execute_reply.started":"2021-09-28T07:33:01.054853Z","shell.execute_reply":"2021-09-28T07:33:01.067807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = multi_network()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:01.06962Z","iopub.execute_input":"2021-09-28T07:33:01.0701Z","iopub.status.idle":"2021-09-28T07:33:01.092787Z","shell.execute_reply.started":"2021-09-28T07:33:01.070059Z","shell.execute_reply":"2021-09-28T07:33:01.091825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**loss 函数**\n\n损失函数表示当前输出和理想输出的差距，越小越好，具体的损失函数多种多样\n\npytorch提供了很多损失函数https://pytorch.org/docs/0.3.0/nn.html#loss-functions\n\n我们选取最常用的交叉熵","metadata":{}},{"cell_type":"markdown","source":"**优化算法**\n\n优化算法参考最优化理论一课知识\n\n常见的优化算法有：\n\nGSD\n\n动量法\n\nAdagrad\n\nRMSProp\n\nAdadelta\n\nAdam","metadata":{}},{"cell_type":"code","source":"# 使用交叉熵 \ncriterion = nn.CrossEntropyLoss()\n\n# 使用随机梯度下降，学习率 0.1\noptimizer = torch.optim.SGD(net.parameters(), 1e-1) ","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:01.094252Z","iopub.execute_input":"2021-09-28T07:33:01.094576Z","iopub.status.idle":"2021-09-28T07:33:01.100275Z","shell.execute_reply.started":"2021-09-28T07:33:01.094534Z","shell.execute_reply":"2021-09-28T07:33:01.099272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 编写训练函数\nfrom datetime import datetime\n\n# 计算准确率\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().data\n    return num_correct / total\n\n\ndef train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n    \n#     如果可以使用gpu，则使用gpu\n    if torch.cuda.is_available():\n        net = net.cuda()\n#     单轮训练开始时间\n    prev_time = datetime.now()\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            if torch.cuda.is_available():\n                im = Variable(im.cuda())  # (bs, 3, h, w)\n                label = Variable(label.cuda())  # (bs, h, w)\n            else:\n                im = Variable(im)\n                label = Variable(label)\n            # forward\n            output = net(im)\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.data\n            train_acc += get_acc(output, label)\n\n#         单轮训练结束时间\n        cur_time = datetime.now()\n        \n        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n        m, s = divmod(remainder, 60)\n        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n        \n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            for im, label in valid_data:\n                if torch.cuda.is_available():\n                    im = Variable(im.cuda(), volatile=True)\n                    label = Variable(label.cuda(), volatile=True)\n                else:\n                    im = Variable(im, volatile=True)\n                    label = Variable(label, volatile=True)\n                output = net(im)\n                loss = criterion(output, label)\n                valid_loss += loss.data\n                valid_acc += get_acc(output, label)\n            epoch_str = (\n                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n                % (epoch, train_loss / len(train_data),\n                   train_acc / len(train_data), valid_loss / len(valid_data),\n                   valid_acc / len(valid_data)))\n        else:\n            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %\n                         (epoch, train_loss / len(train_data),\n                          train_acc / len(train_data)))\n        prev_time = cur_time\n        print(epoch_str + time_str)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:01.101552Z","iopub.execute_input":"2021-09-28T07:33:01.101764Z","iopub.status.idle":"2021-09-28T07:33:01.121907Z","shell.execute_reply.started":"2021-09-28T07:33:01.101741Z","shell.execute_reply":"2021-09-28T07:33:01.120998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(net, train_data, test_data, 10, optimizer, criterion)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:33:01.123272Z","iopub.execute_input":"2021-09-28T07:33:01.123543Z","iopub.status.idle":"2021-09-28T07:34:21.289441Z","shell.execute_reply.started":"2021-09-28T07:33:01.123502Z","shell.execute_reply":"2021-09-28T07:34:21.288477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 打出 moving_mean 的前 10 项\nprint(net.moving_mean[:10])","metadata":{"execution":{"iopub.status.busy":"2021-09-28T07:34:21.290594Z","iopub.execute_input":"2021-09-28T07:34:21.290916Z","iopub.status.idle":"2021-09-28T07:34:21.311043Z","shell.execute_reply.started":"2021-09-28T07:34:21.290886Z","shell.execute_reply":"2021-09-28T07:34:21.310373Z"},"trusted":true},"execution_count":null,"outputs":[]}]}