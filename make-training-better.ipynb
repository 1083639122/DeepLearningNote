{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **数据增强、正则化、学习率衰减**\n**以resnet为例**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"数据增强","metadata":{}},{"cell_type":"markdown","source":"数据增强与数字图像处理有着相似之处，不妨以数字图像处理领域常用的美女蕾娜照片来研究","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport requests\nfrom io import BytesIO\nresponse = requests.get('https://github.com/1083639122/pictures_data/blob/main/lenna.png?raw=true')\nim = Image.open(BytesIO(response.content))\nim","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:34.662827Z","iopub.execute_input":"2021-10-09T11:33:34.663296Z","iopub.status.idle":"2021-10-09T11:33:35.784412Z","shell.execute_reply.started":"2021-10-09T11:33:34.663135Z","shell.execute_reply":"2021-10-09T11:33:35.783805Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms as tfs","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:35.785762Z","iopub.execute_input":"2021-10-09T11:33:35.786527Z","iopub.status.idle":"2021-10-09T11:33:40.116850Z","shell.execute_reply.started":"2021-10-09T11:33:35.786474Z","shell.execute_reply":"2021-10-09T11:33:40.116118Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 比例缩放\nprint('before scale, shape: {}'.format(im.size))\nnew_im = tfs.Resize((200, 200))(im)\nprint('after scale, shape: {}'.format(new_im.size))\nnew_im","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:40.118096Z","iopub.execute_input":"2021-10-09T11:33:40.118336Z","iopub.status.idle":"2021-10-09T11:33:40.150968Z","shell.execute_reply.started":"2021-10-09T11:33:40.118307Z","shell.execute_reply":"2021-10-09T11:33:40.150190Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 随机裁剪出 100 x 100 的区域\nrandom_im1 = tfs.RandomCrop(100)(im)\nrandom_im1","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:40.152283Z","iopub.execute_input":"2021-10-09T11:33:40.152541Z","iopub.status.idle":"2021-10-09T11:33:40.177324Z","shell.execute_reply.started":"2021-10-09T11:33:40.152510Z","shell.execute_reply":"2021-10-09T11:33:40.176603Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 中心裁剪出 100 x 100 的区域\ncenter_im = tfs.CenterCrop(100)(im)\ncenter_im","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:40.179664Z","iopub.execute_input":"2021-10-09T11:33:40.179907Z","iopub.status.idle":"2021-10-09T11:33:40.189275Z","shell.execute_reply.started":"2021-10-09T11:33:40.179876Z","shell.execute_reply":"2021-10-09T11:33:40.188518Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 随机水平翻转\nh_filp = tfs.RandomHorizontalFlip()(im)\nh_filp","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:40.190919Z","iopub.execute_input":"2021-10-09T11:33:40.191162Z","iopub.status.idle":"2021-10-09T11:33:40.344357Z","shell.execute_reply.started":"2021-10-09T11:33:40.191132Z","shell.execute_reply":"2021-10-09T11:33:40.343818Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# 随机竖直翻转\nv_flip = tfs.RandomVerticalFlip()(im)\nv_flip","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:40.345353Z","iopub.execute_input":"2021-10-09T11:33:40.345724Z","iopub.status.idle":"2021-10-09T11:33:40.491144Z","shell.execute_reply.started":"2021-10-09T11:33:40.345687Z","shell.execute_reply":"2021-10-09T11:33:40.490406Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"rot_im = tfs.RandomRotation(45)(im)\nrot_im","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:40.492364Z","iopub.execute_input":"2021-10-09T11:33:40.492599Z","iopub.status.idle":"2021-10-09T11:33:40.602488Z","shell.execute_reply.started":"2021-10-09T11:33:40.492570Z","shell.execute_reply":"2021-10-09T11:33:40.601740Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# 亮度\nbright_im = tfs.ColorJitter(brightness=1)(im) # 随机从 0 ~ 2 之间亮度变化，1 表示原图\nbright_im","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:40.603742Z","iopub.execute_input":"2021-10-09T11:33:40.603978Z","iopub.status.idle":"2021-10-09T11:33:40.748680Z","shell.execute_reply.started":"2021-10-09T11:33:40.603949Z","shell.execute_reply":"2021-10-09T11:33:40.747947Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# 对比度\ncontrast_im = tfs.ColorJitter(contrast=1)(im) # 随机从 0 ~ 2 之间对比度变化，1 表示原图\ncontrast_im","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:40.750181Z","iopub.execute_input":"2021-10-09T11:33:40.750456Z","iopub.status.idle":"2021-10-09T11:33:40.886177Z","shell.execute_reply.started":"2021-10-09T11:33:40.750422Z","shell.execute_reply":"2021-10-09T11:33:40.885432Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# 颜色\ncolor_im = tfs.ColorJitter(hue=0.5)(im) # 随机从 -0.5 ~ 0.5 之间对颜色变化\ncolor_im","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:40.887508Z","iopub.execute_input":"2021-10-09T11:33:40.887762Z","iopub.status.idle":"2021-10-09T11:33:41.051428Z","shell.execute_reply.started":"2021-10-09T11:33:40.887731Z","shell.execute_reply":"2021-10-09T11:33:41.050683Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"上面我们讲了这么图片增强的方法，其实这些方法都不是孤立起来用的，可以联合起来用，比如先做随机翻转，然后随机截取，再做对比度增强等等，torchvision 里面有个非常方便的函数能够将这些变化合起来，就是 torchvision.transforms.Compose()","metadata":{}},{"cell_type":"code","source":"im_aug = tfs.Compose([\n    tfs.Resize(120),\n    tfs.RandomHorizontalFlip(),\n    tfs.RandomCrop(96),\n    tfs.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5)\n])","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:41.052769Z","iopub.execute_input":"2021-10-09T11:33:41.053007Z","iopub.status.idle":"2021-10-09T11:33:41.058064Z","shell.execute_reply.started":"2021-10-09T11:33:41.052976Z","shell.execute_reply":"2021-10-09T11:33:41.057467Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:41.059624Z","iopub.execute_input":"2021-10-09T11:33:41.060129Z","iopub.status.idle":"2021-10-09T11:33:41.069139Z","shell.execute_reply.started":"2021-10-09T11:33:41.060097Z","shell.execute_reply":"2021-10-09T11:33:41.068099Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"nrows = 3\nncols = 3\nfigsize = (8, 8)\n_, figs = plt.subplots(nrows, ncols, figsize=figsize)\nfor i in range(nrows):\n    for j in range(ncols):\n        figs[i][j].imshow(im_aug(im))\n        figs[i][j].axes.get_xaxis().set_visible(False)\n        figs[i][j].axes.get_yaxis().set_visible(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:41.072805Z","iopub.execute_input":"2021-10-09T11:33:41.073131Z","iopub.status.idle":"2021-10-09T11:33:41.751450Z","shell.execute_reply.started":"2021-10-09T11:33:41.073100Z","shell.execute_reply":"2021-10-09T11:33:41.750672Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"可以看到每次做完增强之后的图片都有一些变化，所以这就是我们前面讲的，增加了一些'新'数据\n\n下面我们使用图像增强进行训练网络","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torchvision.datasets import mnist\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:41.752871Z","iopub.execute_input":"2021-10-09T11:33:41.753146Z","iopub.status.idle":"2021-10-09T11:33:41.758091Z","shell.execute_reply.started":"2021-10-09T11:33:41.753113Z","shell.execute_reply":"2021-10-09T11:33:41.757516Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().data\n    return num_correct / total\n\n\ndef train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n    if torch.cuda.is_available():\n        net = net.cuda()\n    prev_time = datetime.now()\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            if torch.cuda.is_available():\n                im = Variable(im.cuda())  # (bs, 3, h, w)\n                label = Variable(label.cuda())  # (bs, h, w)\n            else:\n                im = Variable(im)\n                label = Variable(label)\n            # forward\n            output = net(im)\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.data\n            train_acc += get_acc(output, label)\n\n        cur_time = datetime.now()\n        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n        m, s = divmod(remainder, 60)\n        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            for im, label in valid_data:\n                if torch.cuda.is_available():\n                    im = Variable(im.cuda())\n                    label = Variable(label.cuda())\n                else:\n                    im = Variable(im)\n                    label = Variable(label)\n                output = net(im)\n                loss = criterion(output, label)\n                valid_loss += loss.data\n                valid_acc += get_acc(output, label)\n            epoch_str = (\n                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n                % (epoch, train_loss / len(train_data),\n                   train_acc / len(train_data), valid_loss / len(valid_data),\n                   valid_acc / len(valid_data)))\n        else:\n            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %\n                         (epoch, train_loss / len(train_data),\n                          train_acc / len(train_data)))\n        prev_time = cur_time\n        print(epoch_str + time_str)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:41.759411Z","iopub.execute_input":"2021-10-09T11:33:41.759945Z","iopub.status.idle":"2021-10-09T11:33:41.777203Z","shell.execute_reply.started":"2021-10-09T11:33:41.759913Z","shell.execute_reply":"2021-10-09T11:33:41.776432Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# 使用数据增强\ndef train_tf(x):\n    im_aug = tfs.Compose([\n        tfs.Resize(120),\n        tfs.RandomHorizontalFlip(),\n        tfs.RandomCrop(96),\n        tfs.ToTensor(),\n        tfs.Normalize(0.5, 0.5)\n    ])\n    x = im_aug(x)\n    return x\n\ndef test_tf(x):\n    im_aug = tfs.Compose([\n        tfs.Resize(96),\n        tfs.ToTensor(),\n        tfs.Normalize(0.5,  0.5)\n    ])\n    x = im_aug(x)\n    return x\n\n\nfrom torchvision.datasets import mnist # 导入 pytorch 内置的 mnist 数据\ntrain_set = mnist.MNIST('./data', train=True, transform=train_tf,download=True) # 重新载入数据集，申明定义的数据变换\ntest_set = mnist.MNIST('./data', train=False, transform=test_tf,download=True)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\ntest_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:41.778684Z","iopub.execute_input":"2021-10-09T11:33:41.778888Z","iopub.status.idle":"2021-10-09T11:33:43.547117Z","shell.execute_reply.started":"2021-10-09T11:33:41.778859Z","shell.execute_reply":"2021-10-09T11:33:43.546386Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"查看一个batch数据的尺寸，便于网络的设计","metadata":{}},{"cell_type":"code","source":"x=torch.tensor([])\nfor i,j in train_data:\n#     print(i[0].shape) \n#     print(i[0])\n    x=i\n    break\nprint(x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:43.548231Z","iopub.execute_input":"2021-10-09T11:33:43.548659Z","iopub.status.idle":"2021-10-09T11:33:43.609048Z","shell.execute_reply.started":"2021-10-09T11:33:43.548606Z","shell.execute_reply":"2021-10-09T11:33:43.608237Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"接下来设计实现一个 residual block","metadata":{}},{"cell_type":"code","source":"# 利用pytorch的卷积函数，固定kernel_size参数，以定义一个3*3的卷积函数\ndef conv3x3(in_channel, out_channel, stride=1):\n    return nn.Conv2d(in_channel, out_channel, 3, stride=stride, padding=1, bias=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:43.610347Z","iopub.execute_input":"2021-10-09T11:33:43.610826Z","iopub.status.idle":"2021-10-09T11:33:43.616192Z","shell.execute_reply.started":"2021-10-09T11:33:43.610789Z","shell.execute_reply":"2021-10-09T11:33:43.615389Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class residual_block(nn.Module):\n    def __init__(self, in_channel, out_channel, same_shape=True):\n        super(residual_block, self).__init__()\n        \n#         如果要保持tensor的shape不变，则卷积核的滑动步长改变为2，可达到一个通道数倍增，图像长宽减半的效果\n        self.same_shape = same_shape\n        stride=1 if self.same_shape else 2\n        \n        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n#         每次卷积后进行一次批标准化\n        self.bn1 = nn.BatchNorm2d(out_channel)\n        \n        self.conv2 = conv3x3(out_channel, out_channel)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        if not self.same_shape:\n            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        out = F.relu(self.bn1(out), True)\n        out = self.conv2(out)\n        out = F.relu(self.bn2(out), True)\n        \n        if not self.same_shape:\n            x = self.conv3(x)\n        return F.relu(x+out, True)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:43.617627Z","iopub.execute_input":"2021-10-09T11:33:43.618155Z","iopub.status.idle":"2021-10-09T11:33:43.628172Z","shell.execute_reply.started":"2021-10-09T11:33:43.618118Z","shell.execute_reply":"2021-10-09T11:33:43.627392Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"ResNet是 residual block 模块的堆叠","metadata":{}},{"cell_type":"code","source":"class resnet(nn.Module):\n    def __init__(self, in_channel, num_classes, verbose=False):\n        super(resnet, self).__init__()\n        self.verbose = verbose\n        \n        self.block1 = nn.Conv2d(in_channel, 32, 3,1)\n        \n        self.block2 = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            residual_block(32, 32),\n            residual_block(32, 32)\n        )\n        \n        self.block3 = nn.Sequential(\n            residual_block(32, 64, False),\n            residual_block(64, 64)\n        )\n        \n        self.block4 = nn.Sequential(\n            residual_block(64, 128, False),\n            residual_block(128, 128)\n        )\n        \n        self.block5 = nn.Sequential(\n            residual_block(128, 256, False),\n            residual_block(256, 256),\n            nn.AvgPool2d(4)\n        )\n        \n        self.classifier = nn.Linear(256, num_classes)\n        \n    def forward(self, x):\n        x=x.float()\n        x = self.block1(x)\n        if self.verbose:\n            print('block 1 output: {}'.format(x.shape))\n        x = self.block2(x)\n        if self.verbose:\n            print('block 2 output: {}'.format(x.shape))\n        x = self.block3(x)\n        if self.verbose:\n            print('block 3 output: {}'.format(x.shape))\n        x = self.block4(x)\n        if self.verbose:\n            print('block 4 output: {}'.format(x.shape))\n        x = self.block5(x)\n        if self.verbose:\n            print('block 5 output: {}'.format(x.shape))\n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:43.629547Z","iopub.execute_input":"2021-10-09T11:33:43.629812Z","iopub.status.idle":"2021-10-09T11:33:43.643242Z","shell.execute_reply.started":"2021-10-09T11:33:43.629779Z","shell.execute_reply":"2021-10-09T11:33:43.642323Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"net = resnet(1, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-1)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:43.644874Z","iopub.execute_input":"2021-10-09T11:33:43.645136Z","iopub.status.idle":"2021-10-09T11:33:43.685399Z","shell.execute_reply.started":"2021-10-09T11:33:43.645104Z","shell.execute_reply":"2021-10-09T11:33:43.684686Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train(net, train_data, test_data, 20, optimizer, criterion)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:33:43.687264Z","iopub.execute_input":"2021-10-09T11:33:43.687713Z","iopub.status.idle":"2021-10-09T11:48:59.640027Z","shell.execute_reply.started":"2021-10-09T11:33:43.687678Z","shell.execute_reply":"2021-10-09T11:48:59.639292Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"数据增强使得学习器训练难度加大了，同时在训练轮数比较少时，测试集准确率就已经很高了\n\n这里由于数据集本身过于简单，不能明显体现出数据增强的效果","metadata":{}},{"cell_type":"markdown","source":"正则化","metadata":{}},{"cell_type":"markdown","source":"正则化是机器学习中提出来的一种方法，有 L1 和 L2 正则化，目前使用较多的是 L2 正则化，引入正则化相当于在 loss 函数上面加上一项，比如\n\n$$\nf = loss + \\lambda \\sum_{p \\in params} ||p||_2^2\n$$","metadata":{}},{"cell_type":"markdown","source":"在 pytorch 中正则项就是通过这种方式来加入的，比如想在随机梯度下降法中使用正则项，或者说权重衰减，`torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=1e-3)` 就可以了，这个 `weight_decay` 系数就是上面公式中的 $\\lambda$，非常方便","metadata":{}},{"cell_type":"code","source":"net = resnet(1, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-1, weight_decay=1e-3)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:52:37.882853Z","iopub.execute_input":"2021-10-09T11:52:37.883212Z","iopub.status.idle":"2021-10-09T11:52:37.914856Z","shell.execute_reply.started":"2021-10-09T11:52:37.883171Z","shell.execute_reply":"2021-10-09T11:52:37.914195Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train(net, train_data, test_data,5, optimizer, criterion)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T11:52:56.946375Z","iopub.execute_input":"2021-10-09T11:52:56.947033Z","iopub.status.idle":"2021-10-09T11:57:02.113985Z","shell.execute_reply.started":"2021-10-09T11:52:56.946997Z","shell.execute_reply":"2021-10-09T11:57:02.112466Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"学习率衰减","metadata":{}},{"cell_type":"markdown","source":"对于基于一阶梯度进行优化的方法而言，开始的时候更新的幅度是比较大的，也就是说开始的学习率可以设置大一点，但是当训练集的 loss 下降到一定程度之后，，使用这个太大的学习率就会导致 loss 一直来回震荡\n\n这个时候就需要对学习率进行衰减已达到 loss 的充分下降，而是用学习率衰减的办法能够解决这个矛盾，学习率衰减就是随着训练的进行不断的减小学习率。\n","metadata":{}},{"cell_type":"code","source":"def set_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr","metadata":{"execution":{"iopub.status.busy":"2021-10-09T12:05:28.256083Z","iopub.execute_input":"2021-10-09T12:05:28.256344Z","iopub.status.idle":"2021-10-09T12:05:28.260534Z","shell.execute_reply.started":"2021-10-09T12:05:28.256316Z","shell.execute_reply":"2021-10-09T12:05:28.259706Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n    if torch.cuda.is_available():\n        net = net.cuda()\n    prev_time = datetime.now()\n    for epoch in range(num_epochs):\n#         加入学习率衰减机制\n        if epoch == 15:\n            set_learning_rate(optimizer, 0.01) # 15次修改学习率为 0.01\n        \n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            if torch.cuda.is_available():\n                im = Variable(im.cuda())  # (bs, 3, h, w)\n                label = Variable(label.cuda())  # (bs, h, w)\n            else:\n                im = Variable(im)\n                label = Variable(label)\n            # forward\n            output = net(im)\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.data\n            train_acc += get_acc(output, label)\n\n        cur_time = datetime.now()\n        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n        m, s = divmod(remainder, 60)\n        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            for im, label in valid_data:\n                if torch.cuda.is_available():\n                    im = Variable(im.cuda())\n                    label = Variable(label.cuda())\n                else:\n                    im = Variable(im)\n                    label = Variable(label)\n                output = net(im)\n                loss = criterion(output, label)\n                valid_loss += loss.data\n                valid_acc += get_acc(output, label)\n            epoch_str = (\n                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n                % (epoch, train_loss / len(train_data),\n                   train_acc / len(train_data), valid_loss / len(valid_data),\n                   valid_acc / len(valid_data)))\n        else:\n            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %\n                         (epoch, train_loss / len(train_data),\n                          train_acc / len(train_data)))\n        prev_time = cur_time\n        print(epoch_str + time_str)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T12:05:38.437391Z","iopub.execute_input":"2021-10-09T12:05:38.437699Z","iopub.status.idle":"2021-10-09T12:05:38.451697Z","shell.execute_reply.started":"2021-10-09T12:05:38.437670Z","shell.execute_reply":"2021-10-09T12:05:38.450581Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"net = resnet(1, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-1, weight_decay=1e-3)\ncriterion = nn.CrossEntropyLoss()\ntrain(net, train_data, test_data,20, optimizer, criterion)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T12:05:41.876118Z","iopub.execute_input":"2021-10-09T12:05:41.876367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"在实际应用中，做学习率衰减之前应该经过充分的训练，然后再做学习率衰减得到更好的结果，有的时候甚至需要做多次学习率衰减\n","metadata":{}}]}